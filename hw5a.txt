import streamlit as st
import pdfplumber
from transformers import pipeline
from pptx import Presentation
from paddleocr import PaddleOCR
from PIL import Image
import numpy as np

st.set_page_config(page_title="AI Generated Detector", layout="centered")

@st.cache_resource
def load_detector(model_name: str = "roberta-base-openai-detector"):
	return pipeline(
		"text-classification",
		model=model_name,
		truncation=True,
	)

@st.cache_resource
def load_ocr():
	return PaddleOCR(lang='ch')

@st.cache_resource
def get_ocr():
	return load_ocr()

ocr = get_ocr()

def detect_ai(text: str, detector):
	"""Return aggregated label, aggregated score, and per-chunk info list.

	per-chunk info: list of dicts {'chunk': str, 'label': str, 'score': float}
	"""
	if not text or not text.strip():
		return "UNKNOWN", 0.0, []
	chunk_size = 512
	chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]
	per_chunk = []
	for chunk in chunks:
		try:
			r = detector(chunk)[0]
			per_chunk.append({
				"chunk": chunk,
				"label": r.get("label", "UNKNOWN"),
				"score": float(r.get("score", 0.0)),
			})
		except Exception:
			per_chunk.append({"chunk": chunk, "label": "ERROR", "score": 0.0})
	if not per_chunk:
		return "UNKNOWN", 0.0, []
	scores = {}
	for p in per_chunk:
		lbl = p["label"]
		scores.setdefault(lbl, []).append(p["score"])
	avg_scores = {lbl: sum(v) / len(v) for lbl, v in scores.items()}
	best_label = max(avg_scores, key=avg_scores.get)
	return best_label, avg_scores[best_label], per_chunk

def extract_ppt_text(file):
	prs = Presentation(file)
	texts = []
	for slide in prs.slides:
		for shape in slide.shapes:
			if hasattr(shape, "text") and shape.text:
				texts.append(shape.text)
	return "\n".join(texts)

def extract_pdf_text(file):
	texts = []
	with pdfplumber.open(file) as pdf:
		for page in pdf.pages:
			t = page.extract_text()
			if t:
				texts.append(t)
	return "\n".join(texts)

def extract_image_text(file):
	img = Image.open(file).convert("RGB")
	result = ocr.ocr(np.array(img))
	texts = []
	for page in result:
		for line in page:
			try:
				texts.append(line[1][0])
			except Exception:
				continue
	return "\n".join(texts)

st.title("ğŸ§  AI åµæ¸¬ç³»çµ±ï¼ˆæ–‡å­— / PPT / PDF / åœ–ç‰‡ï¼‰")

# Model selection UI
st.sidebar.header("æ¨¡å‹è¨­å®š")
model_options = [
	"roberta-base-openai-detector",
	"roberta-large-openai-detector",
	"custom",
]
model_choice = st.sidebar.selectbox("é¸æ“‡æª¢æ¸¬æ¨¡å‹", model_options, index=0)
custom_model = ""
if model_choice == "custom":
	custom_model = st.sidebar.text_input("è‡ªè¨‚æ¨¡å‹åç¨± (Hugging Face)", "")
model_name = custom_model.strip() if model_choice == "custom" and custom_model.strip() else model_choice

# load detector for chosen model
try:
	detector = load_detector(model_name)
except Exception as e:
	st.sidebar.error(f"è¼‰å…¥æ¨¡å‹å¤±æ•—ï¼š{e}")
	detector = load_detector("roberta-base-openai-detector")

mode = st.selectbox("é¸æ“‡è¼¸å…¥é¡å‹", ["æ–‡å­—", "PPTX", "PDF", "åœ–ç‰‡"]) 

content = ""

if mode == "æ–‡å­—":
	content = st.text_area("è¼¸å…¥æ–‡å­—", height=250)

elif mode == "PPTX":
	file = st.file_uploader("ä¸Šå‚³ PPTX", type=["pptx"])
	if file:
		try:
			content = extract_ppt_text(file)
		except Exception as e:
			st.error(f"ç„¡æ³•è§£æ PPTXï¼š{e}")

elif mode == "PDF":
	file = st.file_uploader("ä¸Šå‚³ PDF", type=["pdf"])
	if file:
		try:
			content = extract_pdf_text(file)
		except Exception as e:
			st.error(f"ç„¡æ³•è§£æ PDFï¼š{e}")

elif mode == "åœ–ç‰‡":
	file = st.file_uploader("ä¸Šå‚³åœ–ç‰‡", type=["png", "jpg", "jpeg"])
	if file:
		try:
			content = extract_image_text(file)
			st.image(file, caption="ä¸Šå‚³åœ–ç‰‡")
		except Exception as e:
			st.error(f"ç„¡æ³•è§£æåœ–ç‰‡ï¼š{e}")

if st.button("é–‹å§‹åµæ¸¬"):
	if not content or not content.strip():
		st.warning("æ²’æœ‰å¯ç”¨çš„æ–‡å­—ä¾†æºï¼Œè«‹è¼¸å…¥æ–‡å­—æˆ–ä¸Šå‚³æª”æ¡ˆã€‚")
	else:
		label, score, per_chunk = detect_ai(content, detector)
		st.subheader("ğŸ“Š åµæ¸¬çµæœ")
		st.metric("åˆ¤å®š", label)
		try:
			st.progress(int(score * 100))
		except Exception:
			st.progress(0)
		st.write(f"ä¿¡å¿ƒåˆ†æ•¸ï¼š**{score:.2%}**")

		with st.expander("æ¯å€‹ chunk çš„åˆ¤å®šç´°ç¯€ (é¡¯ç¤ºå‰ 10 å€‹ chunk)"):
			display_chunks = per_chunk[:10]
			for idx, p in enumerate(display_chunks, start=1):
				st.write(f"Chunk {idx}: Label = **{p['label']}**, Score = **{p['score']:.2%}**")
				try:
					st.progress(int(p['score'] * 100))
				except Exception:
					st.progress(0)
			if len(per_chunk) > 10:
				st.write(f"... ç¸½å…± {len(per_chunk)} å€‹ chunkï¼Œåƒ…é¡¯ç¤ºå‰ 10 å€‹ã€‚")
